{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "rYkkjVlz4NRe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import io\n",
        "import copy\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import torchattacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "ImoMXaX3syr3"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "-RQ_vPjl4Qjs",
        "outputId": "0352993c-7b68-407e-fbe7-efae6504c654"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MobileNetV3(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
              "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
              "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (12): Conv2dNormActivation(\n",
              "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
              "    (1): Hardswish()\n",
              "    (2): Dropout(p=0.2, inplace=True)\n",
              "    (3): Linear(in_features=1024, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Load the trained model\n",
        "num_classes = 7\n",
        "preprocessing = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "model = models.mobilenet_v3_small(weights=None).to(device)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(in_features=576, out_features=1024, bias=True),\n",
        "    nn.Hardswish(),\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features=1024, out_features=num_classes, bias=True)\n",
        ").to(device)\n",
        "\n",
        "checkpoint = torch.load('./best_trained_models/best_NORMAL--Mobilenetv3Small.v1_epoch40.pth')\n",
        "\n",
        "if 'module' in list(checkpoint['net'].keys())[0]:\n",
        "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpoint['net'].items()}\n",
        "    model.load_state_dict(new_state_dict)\n",
        "else:\n",
        "    model.load_state_dict(checkpoint['net'])\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "4aac25Bd74zj"
      },
      "outputs": [],
      "source": [
        "# Prepare input data\n",
        "# (Ensure the input data is in the appropriate format expected by the model)\n",
        "\n",
        "trash_test_dataset = torchvision.datasets.ImageFolder('./dataset/trashbox/test', transform=preprocessing)\n",
        "trash_test_loader = torch.utils.data.DataLoader(dataset=trash_test_dataset, shuffle=False, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "PI6S8Tt_9sLh"
      },
      "outputs": [],
      "source": [
        "examples = iter(trash_test_loader)\n",
        "sample, labels = next(examples)\n",
        "sample, labels = sample.to(device), labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# examples = iter(trash_test_loader)\n",
        "# samples, labels = next(examples)\n",
        "\n",
        "# # Ensure that samples are in CPU and convert to numpy\n",
        "# samples = samples.cpu().numpy()\n",
        "\n",
        "# # Create a subplot with 2x2 grid\n",
        "# fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "# # Iterate over the first 4 images in the batch\n",
        "# for i in range(2):\n",
        "#     for j in range(2):\n",
        "#         # Extract an image from the batch\n",
        "#         image = samples[i * 2 + j]\n",
        "\n",
        "#         # Transpose the image from (C, H, W) to (H, W, C)\n",
        "#         image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "#         # Plot the image\n",
        "#         axs[i, j].imshow(image)\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# examples = iter(trash_test_loader)\n",
        "# samples, labels = next(examples)\n",
        "# # Ensure that samples are in CPU and convert to numpy\n",
        "# attack = torchattacks.PGD(model)\n",
        "\n",
        "# samples = attack(samples, labels)\n",
        "\n",
        "# # Ensure that samples are in CPU and convert to numpy\n",
        "# samples = samples.cpu().numpy()\n",
        "\n",
        "# # Create a subplot with 2x2 grid\n",
        "# fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "# # Iterate over the first 4 images in the batch\n",
        "# for i in range(2):\n",
        "#     for j in range(2):\n",
        "#         # Extract an image from the batch\n",
        "#         image = samples[i * 2 + j]\n",
        "\n",
        "#         # Transpose the image from (C, H, W) to (H, W, C)\n",
        "#         image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "#         # Plot the image\n",
        "#         axs[i, j].imshow(image)\n",
        "#         # axs[i, j].axis('off')  # Turn off axis labels\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "W3eruqCj838d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "inference_times = []\n",
        "for images, targets in trash_test_loader:\n",
        "    images, targets = images.to(device), targets.to(device)\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(images)  # Add batch dimension\n",
        "    inference_time = time.time() - start_time\n",
        "    inference_times.append(inference_time)\n",
        "\n",
        "# Calculate statistics\n",
        "avg_inference_time = np.mean(inference_times)\n",
        "max_inference_time = np.max(inference_times)\n",
        "min_inference_time = np.min(inference_times)\n",
        "\n",
        "# Plot the latency distribution\n",
        "plt.hist(inference_times, bins=50, density=True)\n",
        "plt.xlabel('Latency (seconds)')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.title('Latency Distribution')\n",
        "\n",
        "plt.text(0.5, 0.95, f'Average: {avg_inference_time:.4f} s', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "plt.text(0.5, 0.90, f'Highest: {max_inference_time:.4f} s', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "plt.text(0.5, 0.85, f'Lowest: {min_inference_time:.4f} s', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "3Q-COcRq4dUv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory Footprint: 1107.01025390625 MB\n",
            "Inference Time: 0.0069768428802490234 seconds\n"
          ]
        }
      ],
      "source": [
        "# Measure memory footprint\n",
        "with torch.no_grad():\n",
        "    predictions = model(sample)\n",
        "torch.cuda.reset_peak_memory_stats(device=device)\n",
        "print(\"Memory Footprint:\", torch.cuda.max_memory_allocated(device=device) / (1024 ** 2), \"MB\")\n",
        "\n",
        "# Measure Inference time\n",
        "start_time = time.time()\n",
        "predictions = model(sample)\n",
        "inference_time = time.time() - start_time\n",
        "print(\"Inference Time:\", inference_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "yxy1MIZV7Oov"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS Throughput: 7744.578530155436\n"
          ]
        }
      ],
      "source": [
        "# Prepare input data (batch of images)\n",
        "# (Ensure the input data is in the appropriate format expected by the model)\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Measure inference time for a batch of images\n",
        "total_time = 0.0\n",
        "num_iterations = 100\n",
        "for _ in range(num_iterations):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(sample)\n",
        "    inference_time = time.time() - start_time\n",
        "    total_time += inference_time\n",
        "\n",
        "# Compute FPS throughput\n",
        "average_inference_time = total_time / num_iterations\n",
        "fps = batch_size / average_inference_time\n",
        "print(\"FPS Throughput:\", fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "SdFEiobW-ZPo"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[88], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrash_test_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\torchvision\\datasets\\folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Acer\\.conda\\envs\\thesisaimodel\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "val_labels = []\n",
        "predicted_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in trash_test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        predictions = model(images)\n",
        "        _, preds = torch.max(predictions, 1)\n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "        predicted_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(val_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.imshow(conf_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, f'{conf_matrix[i, j]:.0f}\\n({conf_matrix[i, j] / np.sum(conf_matrix[i]):.2%})',\n",
        "            horizontalalignment='center', color='white' if conf_matrix[i, j] > conf_matrix.max() / 2 else 'black')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the number of parameters\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of parameters:\", num_params)\n",
        "\n",
        "# Dummy inference to measure execution time\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    start_time = time.time()\n",
        "    output = model(sample)\n",
        "    end_time = time.time()\n",
        "\n",
        "# Calculate GFLOPs\n",
        "execution_time = end_time - start_time\n",
        "flops = num_params * 2  # Assuming one FLOP for each parameter multiplication and addition\n",
        "gflops = (flops / execution_time) / 1e9  # Divide by 1e9 to convert to GFLOPs\n",
        "print(\"GFLOPs:\", gflops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E7_fBLitLS3"
      },
      "outputs": [],
      "source": [
        "# model.eval()  # Set the model to evaluation mode\n",
        "# val_labels = []\n",
        "# predicted_labels = []\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in trash_test_loader:\n",
        "#         images = images.to(device)\n",
        "#         predictions = model(images)\n",
        "#         _, preds = torch.max(predictions, 1)\n",
        "#         val_labels.extend(labels.cpu().numpy())\n",
        "#         predicted_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# # Create confusion matrix\n",
        "# conf_matrix = confusion_matrix(val_labels, predicted_labels)\n",
        "\n",
        "# # Plot confusion matrix\n",
        "# plt.imshow(conf_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.colorbar()\n",
        "# plt.xlabel('Predicted Label')\n",
        "# plt.ylabel('True Label')\n",
        "\n",
        "# # for i in range(conf_matrix.shape[0]):\n",
        "# #     for j in range(conf_matrix.shape[1]):\n",
        "# #         plt.text(j, i, f'{conf_matrix[i, j]:.0f}\\n({conf_matrix[i, j] / np.sum(conf_matrix[i]):.2%})',\n",
        "# #             horizontalalignment='center', color='white' if conf_matrix[i, j] > conf_matrix.max() / 2 else 'black')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ0Tv4bjfJ2c"
      },
      "outputs": [],
      "source": [
        "import torchattacks\n",
        "# Attack hyperparams\n",
        "\n",
        "epsilon = 8.0 / 255\n",
        "alpha = 2.0 / 255\n",
        "k = 10\n",
        "beta = 1\n",
        "\n",
        "# adversary = torchattacks.PGD(model, eps=epsilon, alpha=alpha, steps=k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Setup Adversarial attack \n",
        "\n",
        "attack = torchattacks.PGD(model=model, eps=epsilon, alpha=alpha, steps=k)\n",
        "\n",
        "# class AttackPGD(nn.Module):\n",
        "#     def __init__(self, basic_net, config):\n",
        "#         super(AttackPGD, self).__init__()\n",
        "#         self.basic_net = basic_net\n",
        "#         self.step_size = config['step_size']\n",
        "#         self.epsilon = config['epsilon']\n",
        "#         self.num_steps = config['num_steps']\n",
        "\n",
        "#     def forward(self, inputs, targets):\n",
        "#         x = inputs.detach()\n",
        "#         x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
        "#         for i in range(self.num_steps):\n",
        "#             x.requires_grad_()\n",
        "#             with torch.enable_grad():\n",
        "#                 loss = F.cross_entropy(self.basic_net(x), targets, size_average=False)\n",
        "#             grad = torch.autograd.grad(loss, [x])[0]\n",
        "#             x = x.detach() + self.step_size*torch.sign(grad.detach())\n",
        "#             x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
        "#             x = torch.clamp(x, 0.0, 1.0)\n",
        "#         return self.basic_net(x), x\n",
        "    \n",
        "# config = {\n",
        "#     'step_size' : alpha,\n",
        "#     'epsilon' : epsilon, \n",
        "#     'num_steps' : k\n",
        "# }\n",
        "# net = AttackPGD(model, config)\n",
        "\n",
        "# if 'module' in list(checkpoint['second_net'].keys())[0]:\n",
        "#     new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpoint['second_net'].items()}\n",
        "#     net.load_state_dict(new_state_dict)\n",
        "# else:\n",
        "#     net.load_state_dict(checkpoint['second_net'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "val_labels = []\n",
        "predicted_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in trash_test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        with torch.enable_grad():\n",
        "            adv_images = attack(images, labels)\n",
        "        predictions = model(adv_images)\n",
        "        _, preds = torch.max(predictions, 1)\n",
        "        val_labels.extend(labels.cpu().numpy().tolist())\n",
        "        predicted_labels.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(val_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.imshow(conf_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, f'{conf_matrix[i, j]:.0f}\\n({conf_matrix[i, j] / np.sum(conf_matrix[i]):.2%})',\n",
        "            horizontalalignment='center', color='white' if conf_matrix[i, j] > conf_matrix.max() / 2 else 'black')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPSZgDUgT7GW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, precision_score, precision_recall_curve, recall_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Assuming you have test_loader containing test data\n",
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "    all_probs = []\n",
        "    all_adv_predictions = []\n",
        "    all_adv_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "            \n",
        "            with torch.enable_grad():\n",
        "                adv_images = attack(inputs, targets)\n",
        "            adv_output = model(adv_images)\n",
        "            # adv_outputs = model(adv)\n",
        "\n",
        "            _, adv_predicted = adv_output.max(1)\n",
        "            all_adv_predictions.extend(adv_predicted.cpu().numpy())\n",
        "            all_adv_probs.extend(torch.softmax(adv_output, dim=1).cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
        "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
        "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
        "    adv_f1 = f1_score(all_targets, all_adv_predictions, average='macro')\n",
        "    adv_precision = precision_score(all_targets, all_adv_predictions, average='macro')\n",
        "\n",
        "    # Calculate top-1 accuracy\n",
        "    top1_accuracy = sum(1 for x, y in zip(all_predictions, all_targets) if x == y) / len(all_predictions)\n",
        "    top1_adv_accuracy = sum(1 for x, y in zip(all_adv_predictions, all_targets) if x == y) / len(all_predictions)\n",
        "\n",
        "\n",
        "    # Compute precision and recall for each class\n",
        "    all_targets = np.array(all_targets)\n",
        "    all_probs = np.array(all_probs)\n",
        "    precision_curve = dict()\n",
        "    recall_curve = dict()\n",
        "    for i in range(all_probs.shape[1]):\n",
        "        precision_curve[i], recall_curve[i], _ = precision_recall_curve(all_targets == i, all_probs[:, i])\n",
        "\n",
        "    # Compute ROC curve for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(all_probs.shape[1]):\n",
        "        fpr[i], tpr[i], _ = roc_curve(all_targets == i, all_probs[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    return f1, precision, top1_accuracy, fpr, tpr, roc_auc, precision_curve, recall_curve, top1_adv_accuracy, adv_f1, adv_precision, recall\n",
        "\n",
        "# Call the evaluate_model function with your trained model and test_loader\n",
        "f1, precision, top1_accuracy, fpr, tpr, roc_auc, precision_curve, recall_curve, top1_adv_accuracy, adv_f1, adv_precision, recall = evaluate_model(model, trash_test_loader)\n",
        "\n",
        "# Plot all ROC curves in a single graph\n",
        "plt.figure()\n",
        "for i in range(len(fpr)):\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label='Class {} (AUC = {:.2f})'.format(i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for all classes')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Plot all precision-recall curves in a single graph\n",
        "plt.figure()\n",
        "for i in range(len(precision_curve)):\n",
        "    plt.plot(recall_curve[i], precision_curve[i], lw=2, label='Class {}'.format(i))\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for all classes')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "# Print other metrics\n",
        "print('F1 score:', f1)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('Top-1 accuracy:', top1_accuracy)\n",
        "print('Top-1 adv_accuracy:', top1_adv_accuracy)\n",
        "print('adversarial F1 score:', adv_f1)\n",
        "print('adversarial Precision:', adv_precision)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
